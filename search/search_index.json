{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to PyTorch GAN Trainer \u00b6 Quickly train a GAN to generate images! A simple module for you to directly import and start training different GAN models. You can also log your runs with Weight and Biases. Aim \u00b6 To create an easy inference to train/save/load different GAN models. To learn how to create a Python package. To learn unit testing with PyTest. To learn how to create documentation with MkDocs. How to install? \u00b6 Using pip, run the following command pip install git+https://github.com/kad99kev/pytorch_gan_trainer.git How to use? \u00b6 Please check the Colaboratory notebook to see different examples - Example Notebook You can also visit the documented example here! Contributing \u00b6 If you find any bugs, feel free to raise an issue! If you'd like to contribute by adding other GAN architectures, raise a pull request! Note \u00b6 Since this project was mainly built as a learning experience, there might be few lingering bugs here and there. In that case, please raise an issue or submit a PR! License \u00b6 This project is licensed under the MIT License.","title":"Home"},{"location":"index.html#welcome-to-pytorch-gan-trainer","text":"Quickly train a GAN to generate images! A simple module for you to directly import and start training different GAN models. You can also log your runs with Weight and Biases.","title":"Welcome to PyTorch GAN Trainer"},{"location":"index.html#aim","text":"To create an easy inference to train/save/load different GAN models. To learn how to create a Python package. To learn unit testing with PyTest. To learn how to create documentation with MkDocs.","title":"Aim"},{"location":"index.html#how-to-install","text":"Using pip, run the following command pip install git+https://github.com/kad99kev/pytorch_gan_trainer.git","title":"How to install?"},{"location":"index.html#how-to-use","text":"Please check the Colaboratory notebook to see different examples - Example Notebook You can also visit the documented example here!","title":"How to use?"},{"location":"index.html#contributing","text":"If you find any bugs, feel free to raise an issue! If you'd like to contribute by adding other GAN architectures, raise a pull request!","title":"Contributing"},{"location":"index.html#note","text":"Since this project was mainly built as a learning experience, there might be few lingering bugs here and there. In that case, please raise an issue or submit a PR!","title":"Note"},{"location":"index.html#license","text":"This project is licensed under the MIT License.","title":"License"},{"location":"api/acgan.html","text":"Auxillary Classifier Generative Adveraisal Network. Parameters: Name Type Description Default target_size int Target size for the image to be generated. required num_channels int Number of channels in the dataset image. required num_classes int Number of classes in the dataset. required latent_size int Size of the noise. Default: 100. required generator_feature_size int Number of features for the Generator. required discriminator_feature_size int Number of features for the Discriminator. required g_lr float Learning rate for the Generator. required g_betas tuple Co-efficients for the Generator. required d_lr float Learning rate for the Discriminator. required d_betas float Co-efficients for the Discriminator. required generate ( self , labels , inputs = None , output_type = 'tensor' ) \u00b6 Generate images for given labels and inputs. Parameters: Name Type Description Default labels torch.Tensor A tensor of labels for which the model should generate. required inputs None or torch.Tensor Either give a predefined set of inputs or generate randomly. None output_type str Whether to return a tensor of outputs or a reshaped grid. 'tensor' Returns: Type Description torch.Tensor Depending on output_type, either the raw output tensors or a tensor grid will be returned. Source code in pytorch_gan_trainer/models/acgan/acgan.py def generate ( self , labels , inputs = None , output_type = \"tensor\" ): \"\"\"Generate images for given labels and inputs. Arguments: labels (torch.Tensor): A tensor of labels for which \\ the model should generate. inputs (None or torch.Tensor): Either give a predefined \\ set of inputs or generate randomly. output_type (str): Whether to return a tensor \\ of outputs or a reshaped grid. Returns: torch.Tensor: Depending on output_type, either the raw output tensors \\ or a tensor grid will be returned. \"\"\" if inputs is None : inputs = torch . randn ( size = ( labels . size ( 0 ), self . latent_size )) . to ( self . device ) self . generator . eval () with torch . no_grad (): outputs = self . generator ( inputs , labels ) self . generator . train () if output_type == \"tensor\" : return outputs if output_type == \"image\" : return torchvision . utils . make_grid ( outputs . cpu (), normalize = True ) raise Exception ( \"Invalid return type specified\" ) load_checkpoint ( self , models_path ) \u00b6 Load a previously saved checkpoint. Parameters: Name Type Description Default models_path str Path to load the previous state. required Returns: Type Description int Last processed epoch. Source code in pytorch_gan_trainer/models/acgan/acgan.py def load_checkpoint ( self , models_path ): \"\"\"Load a previously saved checkpoint. Arguments: models_path (str): Path to load the previous state. Returns: int: Last processed epoch. \"\"\" state = torch . load ( models_path , map_location = self . device ) self . generator . load_state_dict ( state [ \"generator\" ]) self . discriminator . load_state_dict ( state [ \"discriminator\" ]) self . g_optim . load_state_dict ( state [ \"g_optim\" ]) self . d_optim . load_state_dict ( state [ \"d_optim\" ]) return state [ \"epoch\" ] + 1 save_checkpoint ( self , epoch , models_path ) \u00b6 Creates a checkpoint of the models and optimizers. Parameters: Name Type Description Default epoch int Current epoch. required models_path str Path to save current state. required Source code in pytorch_gan_trainer/models/acgan/acgan.py def save_checkpoint ( self , epoch , models_path ): \"\"\"Creates a checkpoint of the models and optimizers. Arguments: epoch (int): Current epoch. models_path (str): Path to save current state. \"\"\" torch . save ( { \"epoch\" : epoch , \"generator\" : self . generator . state_dict (), \"discriminator\" : self . discriminator . state_dict (), \"g_optim\" : self . g_optim . state_dict (), \"d_optim\" : self . d_optim . state_dict (), }, models_path , ) set_device ( self , device ) \u00b6 Changes the device on which the models reside. Parameters: Name Type Description Default device torch.device Device to which the models should switch. required Source code in pytorch_gan_trainer/models/acgan/acgan.py def set_device ( self , device ): \"\"\"Changes the device on which the models reside. Arguments: device (torch.device): Device to which the models should switch. \"\"\" self . device = device self . generator . to ( device ) self . discriminator . to ( device ) train ( self , epochs , dataloader , epoch_start = 0 , output_batch = 64 , output_epochs = 1 , output_path = './outputs' , project = None , name = None , config = {}, models_path = None ) \u00b6 Training loop for ACGAN. Parameters: Name Type Description Default epochs str Number of epochs for training. required dataloader torch.utils.data.DataLoader PyTorch DataLoader containing the dataset. required epoch_start int The epoch from which training should start. 0 output_batch int The batch size for the outputs. 64 output_epochs int The frequency for which outputs will be generated (per epoch). 1 output_path str The location at which the outputs will be saved. If output_path is wandb, then Weights and Biases will be configured. './outputs' project str Project name (Weights and Biases only). None name str Experiment name (Weights and Biases only). None config dict Dictionary containing the configuration settings. {} models_path str Path at which (if provided) the checkpoints will be saved. None Source code in pytorch_gan_trainer/models/acgan/acgan.py def train ( self , epochs , dataloader , epoch_start = 0 , output_batch = 64 , output_epochs = 1 , output_path = \"./outputs\" , project = None , name = None , config = {}, models_path = None , ): \"\"\"Training loop for ACGAN. Arguments: epochs (str): Number of epochs for training. dataloader (torch.utils.data.DataLoader): \\ PyTorch DataLoader containing the dataset. epoch_start (int): The epoch from which training should start. output_batch (int): The batch size for the outputs. output_epochs (int): The frequency for which outputs \\ will be generated (per epoch). output_path (str): The location at which the outputs will be saved. \\ If output_path is wandb, then Weights and Biases will be configured. project (str): Project name (Weights and Biases only). name (str): Experiment name (Weights and Biases only). config (dict): Dictionary containing the configuration settings. models_path (str): Path at which (if provided) \\ the checkpoints will be saved. \"\"\" if output_path == \"wandb\" : if project is None : raise Exception ( \"No project name specified\" ) authorize_wandb ( project , name , config ) adversarial_loss = nn . BCELoss () . to ( self . device ) auxillary_loss = nn . CrossEntropyLoss () . to ( self . device ) # Fixed input noise fixed_noise = torch . randn ( size = ( self . num_classes , self . latent_size )) . to ( self . device ) fixed_labels = torch . tensor ([ i for i in range ( self . num_classes )]) . to ( self . device ) # Set tdqm for epoch progress pbar = tqdm () epoch_end = epochs + epoch_start for epoch in range ( epoch_start , epoch_end ): print ( f \"Epoch: { epoch + 1 } / { epoch_end } \" ) pbar . reset ( total = len ( dataloader )) # Setting up losses discriminator_total_losses = [] generator_total_losses = [] accuracy_history = [] for real_images , real_labels in dataloader : # Current batch size current_batch_size = real_images . size ()[ 0 ] # Convert to cuda real_images = real_images . to ( self . device ) real_labels = real_labels . to ( self . device ) # For real vs fake real_validity = torch . ones ( current_batch_size , 1 ) . to ( self . device ) fake_validity = torch . zeros ( current_batch_size , 1 ) . to ( self . device ) # Training Generator self . generator . zero_grad () # Generate fake images input_noise = torch . randn ( size = ( current_batch_size , self . latent_size ) ) . to ( self . device ) fake_labels = torch . randint ( self . num_classes , size = ( current_batch_size ,) ) . to ( self . device ) fake_images = self . generator ( input_noise , fake_labels ) # Calculate Generator loss ( discriminator_fake_validity , discriminator_fake_labels , ) = self . discriminator ( fake_images ) generator_total_loss = ( adversarial_loss ( discriminator_fake_validity , real_validity ) + auxillary_loss ( discriminator_fake_labels , fake_labels ) ) / 2 generator_total_loss . backward () self . g_optim . step () generator_total_losses . append ( generator_total_loss ) # Training Discriminator self . discriminator . zero_grad () # Loss for real images ( discriminator_real_validity , discriminator_real_labels , ) = self . discriminator ( real_images ) discriminator_real_loss = ( adversarial_loss ( discriminator_real_validity , real_validity ) + auxillary_loss ( discriminator_real_labels , real_labels ) ) / 2 # Loss for fake images ( discriminator_fake_validity , discriminator_fake_labels , ) = self . discriminator ( fake_images . detach ()) discriminator_fake_loss = ( adversarial_loss ( discriminator_fake_validity , fake_validity ) + auxillary_loss ( discriminator_fake_labels , fake_labels ) ) / 2 # Total loss discriminator_total_loss = ( discriminator_real_loss + discriminator_fake_loss ) discriminator_total_loss . backward () self . d_optim . step () discriminator_total_losses . append ( discriminator_total_loss ) # Calculate Discriminator Accuracy predictions = np . concatenate ( [ discriminator_real_labels . data . cpu () . numpy (), discriminator_fake_labels . data . cpu () . numpy (), ], axis = 0 , ) true_values = np . concatenate ( [ real_labels . cpu () . numpy (), fake_labels . cpu () . numpy ()], axis = 0 ) discriminator_accuracy = np . mean ( np . argmax ( predictions , axis = 1 ) == true_values ) accuracy_history . append ( discriminator_accuracy ) # Update tqdm pbar . update () d_total_loss = torch . mean ( torch . FloatTensor ( discriminator_total_losses )) accuracy = np . mean ( accuracy_history ) g_total_loss = torch . mean ( torch . FloatTensor ( generator_total_losses )) print ( \"Discriminator Total Loss: {:.3f} , Discriminator Accuracy: {:.3f} , \\ Generator Total Loss: {:.3f} \" . format ( d_total_loss , accuracy , g_total_loss ) ) if output_path == \"wandb\" : log_wandb ( { \"Discriminator Total Loss\" : d_total_loss , \"Discriminator Accuracy\" : accuracy , \"Generator Total Loss\" : g_total_loss , }, epoch + 1 , ) if ( epoch + 1 ) % output_epochs == 0 : save_output ( epoch + 1 , output_path , fixed_noise , self . generator , fixed_labels ) if models_path : self . save_checkpoint ( epoch , models_path ) pbar . refresh ()","title":"ACGAN"},{"location":"api/acgan.html#pytorch_gan_trainer.models.acgan.acgan.ACGAN.generate","text":"Generate images for given labels and inputs. Parameters: Name Type Description Default labels torch.Tensor A tensor of labels for which the model should generate. required inputs None or torch.Tensor Either give a predefined set of inputs or generate randomly. None output_type str Whether to return a tensor of outputs or a reshaped grid. 'tensor' Returns: Type Description torch.Tensor Depending on output_type, either the raw output tensors or a tensor grid will be returned. Source code in pytorch_gan_trainer/models/acgan/acgan.py def generate ( self , labels , inputs = None , output_type = \"tensor\" ): \"\"\"Generate images for given labels and inputs. Arguments: labels (torch.Tensor): A tensor of labels for which \\ the model should generate. inputs (None or torch.Tensor): Either give a predefined \\ set of inputs or generate randomly. output_type (str): Whether to return a tensor \\ of outputs or a reshaped grid. Returns: torch.Tensor: Depending on output_type, either the raw output tensors \\ or a tensor grid will be returned. \"\"\" if inputs is None : inputs = torch . randn ( size = ( labels . size ( 0 ), self . latent_size )) . to ( self . device ) self . generator . eval () with torch . no_grad (): outputs = self . generator ( inputs , labels ) self . generator . train () if output_type == \"tensor\" : return outputs if output_type == \"image\" : return torchvision . utils . make_grid ( outputs . cpu (), normalize = True ) raise Exception ( \"Invalid return type specified\" )","title":"generate()"},{"location":"api/acgan.html#pytorch_gan_trainer.models.acgan.acgan.ACGAN.load_checkpoint","text":"Load a previously saved checkpoint. Parameters: Name Type Description Default models_path str Path to load the previous state. required Returns: Type Description int Last processed epoch. Source code in pytorch_gan_trainer/models/acgan/acgan.py def load_checkpoint ( self , models_path ): \"\"\"Load a previously saved checkpoint. Arguments: models_path (str): Path to load the previous state. Returns: int: Last processed epoch. \"\"\" state = torch . load ( models_path , map_location = self . device ) self . generator . load_state_dict ( state [ \"generator\" ]) self . discriminator . load_state_dict ( state [ \"discriminator\" ]) self . g_optim . load_state_dict ( state [ \"g_optim\" ]) self . d_optim . load_state_dict ( state [ \"d_optim\" ]) return state [ \"epoch\" ] + 1","title":"load_checkpoint()"},{"location":"api/acgan.html#pytorch_gan_trainer.models.acgan.acgan.ACGAN.save_checkpoint","text":"Creates a checkpoint of the models and optimizers. Parameters: Name Type Description Default epoch int Current epoch. required models_path str Path to save current state. required Source code in pytorch_gan_trainer/models/acgan/acgan.py def save_checkpoint ( self , epoch , models_path ): \"\"\"Creates a checkpoint of the models and optimizers. Arguments: epoch (int): Current epoch. models_path (str): Path to save current state. \"\"\" torch . save ( { \"epoch\" : epoch , \"generator\" : self . generator . state_dict (), \"discriminator\" : self . discriminator . state_dict (), \"g_optim\" : self . g_optim . state_dict (), \"d_optim\" : self . d_optim . state_dict (), }, models_path , )","title":"save_checkpoint()"},{"location":"api/acgan.html#pytorch_gan_trainer.models.acgan.acgan.ACGAN.set_device","text":"Changes the device on which the models reside. Parameters: Name Type Description Default device torch.device Device to which the models should switch. required Source code in pytorch_gan_trainer/models/acgan/acgan.py def set_device ( self , device ): \"\"\"Changes the device on which the models reside. Arguments: device (torch.device): Device to which the models should switch. \"\"\" self . device = device self . generator . to ( device ) self . discriminator . to ( device )","title":"set_device()"},{"location":"api/acgan.html#pytorch_gan_trainer.models.acgan.acgan.ACGAN.train","text":"Training loop for ACGAN. Parameters: Name Type Description Default epochs str Number of epochs for training. required dataloader torch.utils.data.DataLoader PyTorch DataLoader containing the dataset. required epoch_start int The epoch from which training should start. 0 output_batch int The batch size for the outputs. 64 output_epochs int The frequency for which outputs will be generated (per epoch). 1 output_path str The location at which the outputs will be saved. If output_path is wandb, then Weights and Biases will be configured. './outputs' project str Project name (Weights and Biases only). None name str Experiment name (Weights and Biases only). None config dict Dictionary containing the configuration settings. {} models_path str Path at which (if provided) the checkpoints will be saved. None Source code in pytorch_gan_trainer/models/acgan/acgan.py def train ( self , epochs , dataloader , epoch_start = 0 , output_batch = 64 , output_epochs = 1 , output_path = \"./outputs\" , project = None , name = None , config = {}, models_path = None , ): \"\"\"Training loop for ACGAN. Arguments: epochs (str): Number of epochs for training. dataloader (torch.utils.data.DataLoader): \\ PyTorch DataLoader containing the dataset. epoch_start (int): The epoch from which training should start. output_batch (int): The batch size for the outputs. output_epochs (int): The frequency for which outputs \\ will be generated (per epoch). output_path (str): The location at which the outputs will be saved. \\ If output_path is wandb, then Weights and Biases will be configured. project (str): Project name (Weights and Biases only). name (str): Experiment name (Weights and Biases only). config (dict): Dictionary containing the configuration settings. models_path (str): Path at which (if provided) \\ the checkpoints will be saved. \"\"\" if output_path == \"wandb\" : if project is None : raise Exception ( \"No project name specified\" ) authorize_wandb ( project , name , config ) adversarial_loss = nn . BCELoss () . to ( self . device ) auxillary_loss = nn . CrossEntropyLoss () . to ( self . device ) # Fixed input noise fixed_noise = torch . randn ( size = ( self . num_classes , self . latent_size )) . to ( self . device ) fixed_labels = torch . tensor ([ i for i in range ( self . num_classes )]) . to ( self . device ) # Set tdqm for epoch progress pbar = tqdm () epoch_end = epochs + epoch_start for epoch in range ( epoch_start , epoch_end ): print ( f \"Epoch: { epoch + 1 } / { epoch_end } \" ) pbar . reset ( total = len ( dataloader )) # Setting up losses discriminator_total_losses = [] generator_total_losses = [] accuracy_history = [] for real_images , real_labels in dataloader : # Current batch size current_batch_size = real_images . size ()[ 0 ] # Convert to cuda real_images = real_images . to ( self . device ) real_labels = real_labels . to ( self . device ) # For real vs fake real_validity = torch . ones ( current_batch_size , 1 ) . to ( self . device ) fake_validity = torch . zeros ( current_batch_size , 1 ) . to ( self . device ) # Training Generator self . generator . zero_grad () # Generate fake images input_noise = torch . randn ( size = ( current_batch_size , self . latent_size ) ) . to ( self . device ) fake_labels = torch . randint ( self . num_classes , size = ( current_batch_size ,) ) . to ( self . device ) fake_images = self . generator ( input_noise , fake_labels ) # Calculate Generator loss ( discriminator_fake_validity , discriminator_fake_labels , ) = self . discriminator ( fake_images ) generator_total_loss = ( adversarial_loss ( discriminator_fake_validity , real_validity ) + auxillary_loss ( discriminator_fake_labels , fake_labels ) ) / 2 generator_total_loss . backward () self . g_optim . step () generator_total_losses . append ( generator_total_loss ) # Training Discriminator self . discriminator . zero_grad () # Loss for real images ( discriminator_real_validity , discriminator_real_labels , ) = self . discriminator ( real_images ) discriminator_real_loss = ( adversarial_loss ( discriminator_real_validity , real_validity ) + auxillary_loss ( discriminator_real_labels , real_labels ) ) / 2 # Loss for fake images ( discriminator_fake_validity , discriminator_fake_labels , ) = self . discriminator ( fake_images . detach ()) discriminator_fake_loss = ( adversarial_loss ( discriminator_fake_validity , fake_validity ) + auxillary_loss ( discriminator_fake_labels , fake_labels ) ) / 2 # Total loss discriminator_total_loss = ( discriminator_real_loss + discriminator_fake_loss ) discriminator_total_loss . backward () self . d_optim . step () discriminator_total_losses . append ( discriminator_total_loss ) # Calculate Discriminator Accuracy predictions = np . concatenate ( [ discriminator_real_labels . data . cpu () . numpy (), discriminator_fake_labels . data . cpu () . numpy (), ], axis = 0 , ) true_values = np . concatenate ( [ real_labels . cpu () . numpy (), fake_labels . cpu () . numpy ()], axis = 0 ) discriminator_accuracy = np . mean ( np . argmax ( predictions , axis = 1 ) == true_values ) accuracy_history . append ( discriminator_accuracy ) # Update tqdm pbar . update () d_total_loss = torch . mean ( torch . FloatTensor ( discriminator_total_losses )) accuracy = np . mean ( accuracy_history ) g_total_loss = torch . mean ( torch . FloatTensor ( generator_total_losses )) print ( \"Discriminator Total Loss: {:.3f} , Discriminator Accuracy: {:.3f} , \\ Generator Total Loss: {:.3f} \" . format ( d_total_loss , accuracy , g_total_loss ) ) if output_path == \"wandb\" : log_wandb ( { \"Discriminator Total Loss\" : d_total_loss , \"Discriminator Accuracy\" : accuracy , \"Generator Total Loss\" : g_total_loss , }, epoch + 1 , ) if ( epoch + 1 ) % output_epochs == 0 : save_output ( epoch + 1 , output_path , fixed_noise , self . generator , fixed_labels ) if models_path : self . save_checkpoint ( epoch , models_path ) pbar . refresh ()","title":"train()"},{"location":"api/datasets.html","text":"prepare_dataloader ( dataset_path , image_size , batch_size ) \u00b6 Prepares a PyTorch DataLoader from a given path with a specific image and batch size. If a specific dataset type is given, the path will not be read and a torchvision dataset will be loaded instead. Parameters: Name Type Description Default dataset_path str The location of the dataset. required image_size int The image size of the images in the DataLoader. required batch_size int The batch size to be set for the DataLoader. required Returns: Type Description torch.utils.data.DataLoader A DataLoader with the specified image size and batch size, Length of the number of classes found in the dataset. Source code in pytorch_gan_trainer/datasets.py def prepare_dataloader ( dataset_path , image_size , batch_size ): \"\"\"Prepares a PyTorch DataLoader from a given path \\ with a specific image and batch size. \\ If a specific dataset type is given, \\ the path will not be read and \\ a torchvision dataset will be loaded instead. Arguments: dataset_path (str): The location of the dataset. image_size (int): The image size of the images in the DataLoader. batch_size (int): The batch size to be set for the DataLoader. Returns: torch.utils.data.DataLoader: A DataLoader \\ with the specified image size and batch size, \\ Length of the number of classes found in the dataset. \"\"\" transform = transforms . Compose ( [ transforms . Resize (( image_size , image_size )), transforms . ToTensor (), transforms . Normalize ( mean = ( 0.5 ), std = ( 0.5 )), ] ) if _check_path ( dataset_path ): dataset = torchvision . datasets . ImageFolder ( dataset_path , transform = transform ) else : dataset = _get_torch_dataset ( dataset_path , transform ) dataloader = DataLoader ( dataset , batch_size , shuffle = True , pin_memory = True , num_workers = 4 ) return dataloader , dataset . classes Datasets \u00b6 The supported datasets are: CIFAR MNIST Fashion-MNIST","title":"Datasets"},{"location":"api/datasets.html#pytorch_gan_trainer.datasets.prepare_dataloader","text":"Prepares a PyTorch DataLoader from a given path with a specific image and batch size. If a specific dataset type is given, the path will not be read and a torchvision dataset will be loaded instead. Parameters: Name Type Description Default dataset_path str The location of the dataset. required image_size int The image size of the images in the DataLoader. required batch_size int The batch size to be set for the DataLoader. required Returns: Type Description torch.utils.data.DataLoader A DataLoader with the specified image size and batch size, Length of the number of classes found in the dataset. Source code in pytorch_gan_trainer/datasets.py def prepare_dataloader ( dataset_path , image_size , batch_size ): \"\"\"Prepares a PyTorch DataLoader from a given path \\ with a specific image and batch size. \\ If a specific dataset type is given, \\ the path will not be read and \\ a torchvision dataset will be loaded instead. Arguments: dataset_path (str): The location of the dataset. image_size (int): The image size of the images in the DataLoader. batch_size (int): The batch size to be set for the DataLoader. Returns: torch.utils.data.DataLoader: A DataLoader \\ with the specified image size and batch size, \\ Length of the number of classes found in the dataset. \"\"\" transform = transforms . Compose ( [ transforms . Resize (( image_size , image_size )), transforms . ToTensor (), transforms . Normalize ( mean = ( 0.5 ), std = ( 0.5 )), ] ) if _check_path ( dataset_path ): dataset = torchvision . datasets . ImageFolder ( dataset_path , transform = transform ) else : dataset = _get_torch_dataset ( dataset_path , transform ) dataloader = DataLoader ( dataset , batch_size , shuffle = True , pin_memory = True , num_workers = 4 ) return dataloader , dataset . classes","title":"prepare_dataloader()"},{"location":"api/datasets.html#datasets","text":"The supported datasets are: CIFAR MNIST Fashion-MNIST","title":"Datasets"},{"location":"api/dcgan.html","text":"Deep Convolutional Generative Adveraisal Network. Parameters: Name Type Description Default target_size int Target size for the image to be generated. required num_channels int Number of channels in the dataset image. required latent_size int Size of the noise. required generator_feature_size int Number of features for the Generator. required discriminator_feature_size int Number of features for the Discriminator. required g_lr float Learning rate for the Generator. required g_betas tuple Co-efficients for the Generator. required d_lr float Learning rate for the Discriminator. required d_betas float Co-efficients for the Discriminator. required generate ( self , batch_size , inputs = None , output_type = 'tensor' ) \u00b6 Generate images for given labels and inputs. Parameters: Name Type Description Default batch_size int Batch size of outputs. required inputs None or torch.Tensor Either give a predefined set of inputs or generate randomly. None output_type str Whether to return a tensor of outputs or a reshaped grid. 'tensor' Returns: Type Description torch.Tensor Depending on output_type, either the raw output tensors or a tensor grid will be returned. Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def generate ( self , batch_size , inputs = None , output_type = \"tensor\" ): \"\"\"Generate images for given labels and inputs. Arguments: batch_size (int): Batch size of outputs. inputs (None or torch.Tensor): Either give a predefined \\ set of inputs or generate randomly. output_type (str): Whether to return a tensor of outputs or a reshaped grid. Returns: torch.Tensor: Depending on output_type, either the raw output tensors \\ or a tensor grid will be returned. \"\"\" if inputs is None : inputs = torch . randn ( size = ( batch_size , self . latent_size )) . to ( self . device ) self . generator . eval () with torch . no_grad (): outputs = self . generator ( inputs ) self . generator . train () if output_type == \"tensor\" : return outputs if output_type == \"image\" : return torchvision . utils . make_grid ( outputs . cpu (), normalize = True ) raise Exception ( \"Invalid return type specified\" ) load_checkpoint ( self , models_path ) \u00b6 Load a previously saved checkpoint. Parameters: Name Type Description Default models_path str Path to load the previous state. required Returns: Type Description int Last processed epoch. Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def load_checkpoint ( self , models_path ): \"\"\"Load a previously saved checkpoint. Arguments: models_path (str): Path to load the previous state. Returns: int: Last processed epoch. \"\"\" state = torch . load ( models_path , map_location = self . device ) self . generator . load_state_dict ( state [ \"generator\" ]) self . discriminator . load_state_dict ( state [ \"discriminator\" ]) self . g_optim . load_state_dict ( state [ \"g_optim\" ]) self . d_optim . load_state_dict ( state [ \"d_optim\" ]) return state [ \"epoch\" ] + 1 save_checkpoint ( self , epoch , models_path ) \u00b6 Creates a checkpoint of the models and optimizers. Parameters: Name Type Description Default epoch int Current epoch. required models_path str Path to save current state. required Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def save_checkpoint ( self , epoch , models_path ): \"\"\"Creates a checkpoint of the models and optimizers. Arguments: epoch (int): Current epoch. models_path (str): Path to save current state. \"\"\" torch . save ( { \"epoch\" : epoch , \"generator\" : self . generator . state_dict (), \"discriminator\" : self . discriminator . state_dict (), \"g_optim\" : self . g_optim . state_dict (), \"d_optim\" : self . d_optim . state_dict (), }, models_path , ) set_device ( self , device ) \u00b6 Changes the device on which the models reside. Parameters: Name Type Description Default device torch.device Device to which the models should switch. required Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def set_device ( self , device ): \"\"\"Changes the device on which the models reside. Arguments: device (torch.device): Device to which the models should switch. \"\"\" self . device = device self . generator . to ( device ) self . discriminator . to ( device ) train ( self , epochs , dataloader , epoch_start = 0 , output_batch = 64 , output_epochs = 1 , output_path = './outputs' , project = None , name = None , config = {}, models_path = None ) \u00b6 Training loop for DCGAN. Parameters: Name Type Description Default epochs int Number of epochs for training. required dataloader torch.utils.data.DataLoader PyTorch DataLoader containing the dataset. required epoch_start int The epoch from which training should start. 0 output_batch int The batch size for the outputs. 64 output_epochs int The frequency for which outputs will be generated (per epoch). 1 output_path str The location at which the outputs will be saved. If output_path is wandb, then Weights and Biases will be configured. './outputs' project str Project name (Weights and Biases only). None name str Experiment name (Weights and Biases only). None config dict Dictionary containing the configuration settings. {} models_path str Path at which (if provided) the checkpoints will be saved. None Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def train ( self , epochs , dataloader , epoch_start = 0 , output_batch = 64 , output_epochs = 1 , output_path = \"./outputs\" , project = None , name = None , config = {}, models_path = None , ): \"\"\"Training loop for DCGAN. Arguments: epochs (int): Number of epochs for training. dataloader (torch.utils.data.DataLoader): \\ PyTorch DataLoader containing the dataset. epoch_start (int): The epoch from which training should start. output_batch (int): The batch size for the outputs. output_epochs (int): The frequency for which outputs \\ will be generated (per epoch). output_path (str): The location at which the outputs will be saved. \\ If output_path is wandb, then Weights and Biases will be configured. project (str): Project name (Weights and Biases only). name (str): Experiment name (Weights and Biases only). config (dict): Dictionary containing the configuration settings. models_path (str): Path at which (if provided) \\ the checkpoints will be saved. \"\"\" if output_path == \"wandb\" : if project is None : raise Exception ( \"No project name specified\" ) authorize_wandb ( project , name , config ) adversarial_loss = nn . BCELoss () . to ( self . device ) # Fixed input noise fixed_noise = torch . randn ( size = ( output_batch , self . latent_size )) . to ( self . device ) # Set tdqm for epoch progress pbar = tqdm () epoch_end = epochs + epoch_start for epoch in range ( epoch_start , epoch_end ): print ( f \"Epoch: { epoch + 1 } / { epoch_end } \" ) pbar . reset ( total = len ( dataloader )) # Setting up losses discriminator_total_losses = [] generator_total_losses = [] for real_images , _ in dataloader : # Current batch size current_batch_size = real_images . size ()[ 0 ] # Convert to cuda real_images = real_images . to ( self . device ) # For real vs fake real_labels = torch . ones ( current_batch_size , 1 ) . to ( self . device ) fake_labels = torch . zeros ( current_batch_size , 1 ) . to ( self . device ) # Training Generator self . generator . zero_grad () # Generate fake images input_noise = torch . randn ( size = ( current_batch_size , self . latent_size ) ) . to ( self . device ) fake_images = self . generator ( input_noise ) # Calculate Generator loss discriminator_fake_labels = self . discriminator ( fake_images ) generator_total_loss = adversarial_loss ( discriminator_fake_labels , real_labels ) generator_total_loss . backward () self . g_optim . step () generator_total_losses . append ( generator_total_loss ) # Training Discriminator self . discriminator . zero_grad () # Loss for real images discriminator_real_labels = self . discriminator ( real_images ) discriminator_real_loss = adversarial_loss ( discriminator_real_labels , real_labels ) # Loss for fake images discriminator_fake_labels = self . discriminator ( fake_images . detach ()) discriminator_fake_loss = adversarial_loss ( discriminator_fake_labels , fake_labels ) # Total loss discriminator_total_loss = ( discriminator_real_loss + discriminator_fake_loss ) discriminator_total_loss . backward () self . d_optim . step () discriminator_total_losses . append ( discriminator_total_loss ) # Update tqdm pbar . update () d_total_loss = torch . mean ( torch . FloatTensor ( discriminator_total_losses )) g_total_loss = torch . mean ( torch . FloatTensor ( generator_total_losses )) print ( \"Discriminator Total Loss: {:.3f} , Generator Total Loss: {:.3f} \" . format ( d_total_loss , g_total_loss ) ) if output_path == \"wandb\" : log_wandb ( { \"Discriminator Total Loss\" : d_total_loss , \"Generator Total Loss\" : g_total_loss , }, epoch + 1 , ) if ( epoch + 1 ) % output_epochs == 0 : save_output ( epoch + 1 , output_path , fixed_noise , self . generator ) if models_path : self . save_checkpoint ( epoch , models_path ) pbar . refresh ()","title":"DCGAN"},{"location":"api/dcgan.html#pytorch_gan_trainer.models.dcgan.dcgan.DCGAN.generate","text":"Generate images for given labels and inputs. Parameters: Name Type Description Default batch_size int Batch size of outputs. required inputs None or torch.Tensor Either give a predefined set of inputs or generate randomly. None output_type str Whether to return a tensor of outputs or a reshaped grid. 'tensor' Returns: Type Description torch.Tensor Depending on output_type, either the raw output tensors or a tensor grid will be returned. Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def generate ( self , batch_size , inputs = None , output_type = \"tensor\" ): \"\"\"Generate images for given labels and inputs. Arguments: batch_size (int): Batch size of outputs. inputs (None or torch.Tensor): Either give a predefined \\ set of inputs or generate randomly. output_type (str): Whether to return a tensor of outputs or a reshaped grid. Returns: torch.Tensor: Depending on output_type, either the raw output tensors \\ or a tensor grid will be returned. \"\"\" if inputs is None : inputs = torch . randn ( size = ( batch_size , self . latent_size )) . to ( self . device ) self . generator . eval () with torch . no_grad (): outputs = self . generator ( inputs ) self . generator . train () if output_type == \"tensor\" : return outputs if output_type == \"image\" : return torchvision . utils . make_grid ( outputs . cpu (), normalize = True ) raise Exception ( \"Invalid return type specified\" )","title":"generate()"},{"location":"api/dcgan.html#pytorch_gan_trainer.models.dcgan.dcgan.DCGAN.load_checkpoint","text":"Load a previously saved checkpoint. Parameters: Name Type Description Default models_path str Path to load the previous state. required Returns: Type Description int Last processed epoch. Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def load_checkpoint ( self , models_path ): \"\"\"Load a previously saved checkpoint. Arguments: models_path (str): Path to load the previous state. Returns: int: Last processed epoch. \"\"\" state = torch . load ( models_path , map_location = self . device ) self . generator . load_state_dict ( state [ \"generator\" ]) self . discriminator . load_state_dict ( state [ \"discriminator\" ]) self . g_optim . load_state_dict ( state [ \"g_optim\" ]) self . d_optim . load_state_dict ( state [ \"d_optim\" ]) return state [ \"epoch\" ] + 1","title":"load_checkpoint()"},{"location":"api/dcgan.html#pytorch_gan_trainer.models.dcgan.dcgan.DCGAN.save_checkpoint","text":"Creates a checkpoint of the models and optimizers. Parameters: Name Type Description Default epoch int Current epoch. required models_path str Path to save current state. required Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def save_checkpoint ( self , epoch , models_path ): \"\"\"Creates a checkpoint of the models and optimizers. Arguments: epoch (int): Current epoch. models_path (str): Path to save current state. \"\"\" torch . save ( { \"epoch\" : epoch , \"generator\" : self . generator . state_dict (), \"discriminator\" : self . discriminator . state_dict (), \"g_optim\" : self . g_optim . state_dict (), \"d_optim\" : self . d_optim . state_dict (), }, models_path , )","title":"save_checkpoint()"},{"location":"api/dcgan.html#pytorch_gan_trainer.models.dcgan.dcgan.DCGAN.set_device","text":"Changes the device on which the models reside. Parameters: Name Type Description Default device torch.device Device to which the models should switch. required Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def set_device ( self , device ): \"\"\"Changes the device on which the models reside. Arguments: device (torch.device): Device to which the models should switch. \"\"\" self . device = device self . generator . to ( device ) self . discriminator . to ( device )","title":"set_device()"},{"location":"api/dcgan.html#pytorch_gan_trainer.models.dcgan.dcgan.DCGAN.train","text":"Training loop for DCGAN. Parameters: Name Type Description Default epochs int Number of epochs for training. required dataloader torch.utils.data.DataLoader PyTorch DataLoader containing the dataset. required epoch_start int The epoch from which training should start. 0 output_batch int The batch size for the outputs. 64 output_epochs int The frequency for which outputs will be generated (per epoch). 1 output_path str The location at which the outputs will be saved. If output_path is wandb, then Weights and Biases will be configured. './outputs' project str Project name (Weights and Biases only). None name str Experiment name (Weights and Biases only). None config dict Dictionary containing the configuration settings. {} models_path str Path at which (if provided) the checkpoints will be saved. None Source code in pytorch_gan_trainer/models/dcgan/dcgan.py def train ( self , epochs , dataloader , epoch_start = 0 , output_batch = 64 , output_epochs = 1 , output_path = \"./outputs\" , project = None , name = None , config = {}, models_path = None , ): \"\"\"Training loop for DCGAN. Arguments: epochs (int): Number of epochs for training. dataloader (torch.utils.data.DataLoader): \\ PyTorch DataLoader containing the dataset. epoch_start (int): The epoch from which training should start. output_batch (int): The batch size for the outputs. output_epochs (int): The frequency for which outputs \\ will be generated (per epoch). output_path (str): The location at which the outputs will be saved. \\ If output_path is wandb, then Weights and Biases will be configured. project (str): Project name (Weights and Biases only). name (str): Experiment name (Weights and Biases only). config (dict): Dictionary containing the configuration settings. models_path (str): Path at which (if provided) \\ the checkpoints will be saved. \"\"\" if output_path == \"wandb\" : if project is None : raise Exception ( \"No project name specified\" ) authorize_wandb ( project , name , config ) adversarial_loss = nn . BCELoss () . to ( self . device ) # Fixed input noise fixed_noise = torch . randn ( size = ( output_batch , self . latent_size )) . to ( self . device ) # Set tdqm for epoch progress pbar = tqdm () epoch_end = epochs + epoch_start for epoch in range ( epoch_start , epoch_end ): print ( f \"Epoch: { epoch + 1 } / { epoch_end } \" ) pbar . reset ( total = len ( dataloader )) # Setting up losses discriminator_total_losses = [] generator_total_losses = [] for real_images , _ in dataloader : # Current batch size current_batch_size = real_images . size ()[ 0 ] # Convert to cuda real_images = real_images . to ( self . device ) # For real vs fake real_labels = torch . ones ( current_batch_size , 1 ) . to ( self . device ) fake_labels = torch . zeros ( current_batch_size , 1 ) . to ( self . device ) # Training Generator self . generator . zero_grad () # Generate fake images input_noise = torch . randn ( size = ( current_batch_size , self . latent_size ) ) . to ( self . device ) fake_images = self . generator ( input_noise ) # Calculate Generator loss discriminator_fake_labels = self . discriminator ( fake_images ) generator_total_loss = adversarial_loss ( discriminator_fake_labels , real_labels ) generator_total_loss . backward () self . g_optim . step () generator_total_losses . append ( generator_total_loss ) # Training Discriminator self . discriminator . zero_grad () # Loss for real images discriminator_real_labels = self . discriminator ( real_images ) discriminator_real_loss = adversarial_loss ( discriminator_real_labels , real_labels ) # Loss for fake images discriminator_fake_labels = self . discriminator ( fake_images . detach ()) discriminator_fake_loss = adversarial_loss ( discriminator_fake_labels , fake_labels ) # Total loss discriminator_total_loss = ( discriminator_real_loss + discriminator_fake_loss ) discriminator_total_loss . backward () self . d_optim . step () discriminator_total_losses . append ( discriminator_total_loss ) # Update tqdm pbar . update () d_total_loss = torch . mean ( torch . FloatTensor ( discriminator_total_losses )) g_total_loss = torch . mean ( torch . FloatTensor ( generator_total_losses )) print ( \"Discriminator Total Loss: {:.3f} , Generator Total Loss: {:.3f} \" . format ( d_total_loss , g_total_loss ) ) if output_path == \"wandb\" : log_wandb ( { \"Discriminator Total Loss\" : d_total_loss , \"Generator Total Loss\" : g_total_loss , }, epoch + 1 , ) if ( epoch + 1 ) % output_epochs == 0 : save_output ( epoch + 1 , output_path , fixed_noise , self . generator ) if models_path : self . save_checkpoint ( epoch , models_path ) pbar . refresh ()","title":"train()"},{"location":"example/example.html","text":"This is an example code on how to train a ACGAN in the MNIST Dataset. import torch import torchvision import pytorch_gan_trainer as pgt image_size = 64 batch_size = 64 epochs = 10 num_channels = 1 device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) # Download and prepare dataloader dataloader , classes = pgt . datasets . prepare_dataloader ( \"mnist\" , image_size , batch_size ) # Create model num_classes = len ( classes ) acgan = pgt . models . ACGAN ( image_size , num_channels , num_classes ) # Set device acgan . set_device ( device ) # Train model acgan . train ( epochs , dataloader , models_path = \"saved_models.pt\" ) # Generate images from model labels = torch . tensor ([ i for i in range ( 10 )]) . to ( device ) outputs = acgan . generate ( labels ) # Load model new_acgan = pgt . models . ACGAN ( image_size , num_channels , num_classes ) total_epochs = new_acgan . load_checkpoint ( \"saved_models.pt\" ) # Returns the number of epochs the saved model trained for","title":"Example"}]}